{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n"
     ]
    }
   ],
   "source": [
    "#Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__) #Printing tensorflow version to ensure library correctly installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Heart rate</th>\n",
       "      <th>Systolic blood pressure</th>\n",
       "      <th>Diastolic blood pressure</th>\n",
       "      <th>Blood sugar</th>\n",
       "      <th>CK-MB</th>\n",
       "      <th>Troponin</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>160</td>\n",
       "      <td>83</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.012</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>98</td>\n",
       "      <td>46</td>\n",
       "      <td>296.0</td>\n",
       "      <td>6.75</td>\n",
       "      <td>1.060</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>160</td>\n",
       "      <td>77</td>\n",
       "      <td>270.0</td>\n",
       "      <td>1.99</td>\n",
       "      <td>0.003</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>120</td>\n",
       "      <td>55</td>\n",
       "      <td>270.0</td>\n",
       "      <td>13.87</td>\n",
       "      <td>0.122</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>112</td>\n",
       "      <td>65</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.003</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>112</td>\n",
       "      <td>58</td>\n",
       "      <td>87.0</td>\n",
       "      <td>1.83</td>\n",
       "      <td>0.004</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>179</td>\n",
       "      <td>68</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.003</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>214</td>\n",
       "      <td>82</td>\n",
       "      <td>87.0</td>\n",
       "      <td>300.00</td>\n",
       "      <td>2.370</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>154</td>\n",
       "      <td>81</td>\n",
       "      <td>135.0</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0.004</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>160</td>\n",
       "      <td>95</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.84</td>\n",
       "      <td>0.011</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Gender  Heart rate  Systolic blood pressure  Diastolic blood pressure  \\\n",
       "0   63       1          66                      160                        83   \n",
       "1   20       1          94                       98                        46   \n",
       "2   56       1          64                      160                        77   \n",
       "3   66       1          70                      120                        55   \n",
       "4   54       1          64                      112                        65   \n",
       "5   52       0          61                      112                        58   \n",
       "6   38       0          40                      179                        68   \n",
       "7   61       1          60                      214                        82   \n",
       "8   49       0          60                      154                        81   \n",
       "9   65       1          61                      160                        95   \n",
       "\n",
       "   Blood sugar   CK-MB  Troponin    Result  \n",
       "0        160.0    1.80     0.012  negative  \n",
       "1        296.0    6.75     1.060  positive  \n",
       "2        270.0    1.99     0.003  negative  \n",
       "3        270.0   13.87     0.122  positive  \n",
       "4        300.0    1.08     0.003  negative  \n",
       "5         87.0    1.83     0.004  negative  \n",
       "6        102.0    0.71     0.003  negative  \n",
       "7         87.0  300.00     2.370  positive  \n",
       "8        135.0    2.35     0.004  negative  \n",
       "9        100.0    2.84     0.011  negative  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_medical_data = pd.read_csv('./Medicaldataset.csv') #Initialise dataframe from Medicaldataset.csv\n",
    "df_medical_data.head(10) #Printing the first 10 rows of the dataset (dataset has 1319 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result\n",
       "positive    810\n",
       "negative    509\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_medical_data.Result.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based of the available labels in the 'Result' column we can see that on the 'positive' and 'negative' labels are present hence we are dealing with a binary classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1319, 9)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_medical_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Age' range is [14 , 103]\n",
      "'Gender' range is [0 , 1]\n",
      "'Heart rate' range is [20 , 1111]\n",
      "'Systolic blood pressure' range is [42 , 223]\n",
      "'Diastolic blood pressure' range is [38 , 154]\n",
      "'Blood sugar' range is [35.0 , 541.0]\n",
      "'CK-MB' range is [0.321 , 300.0]\n",
      "'Troponin' range is [0.001 , 10.3]\n"
     ]
    }
   ],
   "source": [
    "def column_min_max(df, column):\n",
    "    min_value = df[column].min()\n",
    "    max_value = df[column].max()\n",
    "    print(f\"'{column}' range is [{min_value} , {max_value}]\")\n",
    "\n",
    "columns = ['Age', 'Gender', 'Heart rate', 'Systolic blood pressure', 'Diastolic blood pressure', 'Blood sugar', 'CK-MB', 'Troponin']\n",
    "\n",
    "for column in columns:\n",
    "    column_min_max(df_medical_data, column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation of data:\n",
    "\n",
    "##### 'Result' converted to 0.0 for negative and 1.0 for positive.\n",
    "\n",
    "##### All other columns aside from 'Gender\" are scaled to between 0.0 and 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reece Lazarus\\AppData\\Local\\Temp\\ipykernel_14708\\4011696001.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_medical_data['Result'] = df_medical_data['Result'].replace({'negative': 0, 'positive': 1})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Heart rate</th>\n",
       "      <th>Systolic blood pressure</th>\n",
       "      <th>Diastolic blood pressure</th>\n",
       "      <th>Blood sugar</th>\n",
       "      <th>CK-MB</th>\n",
       "      <th>Troponin</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.550562</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.042163</td>\n",
       "      <td>0.651934</td>\n",
       "      <td>0.387931</td>\n",
       "      <td>0.247036</td>\n",
       "      <td>0.004935</td>\n",
       "      <td>0.001068</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.067416</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.067828</td>\n",
       "      <td>0.309392</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.515810</td>\n",
       "      <td>0.021453</td>\n",
       "      <td>0.102826</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.471910</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.040330</td>\n",
       "      <td>0.651934</td>\n",
       "      <td>0.336207</td>\n",
       "      <td>0.464427</td>\n",
       "      <td>0.005569</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.584270</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.045830</td>\n",
       "      <td>0.430939</td>\n",
       "      <td>0.146552</td>\n",
       "      <td>0.464427</td>\n",
       "      <td>0.045212</td>\n",
       "      <td>0.011749</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.449438</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.040330</td>\n",
       "      <td>0.386740</td>\n",
       "      <td>0.232759</td>\n",
       "      <td>0.523715</td>\n",
       "      <td>0.002533</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.426966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037580</td>\n",
       "      <td>0.386740</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.102767</td>\n",
       "      <td>0.005035</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.269663</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018332</td>\n",
       "      <td>0.756906</td>\n",
       "      <td>0.258621</td>\n",
       "      <td>0.132411</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.528090</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.036664</td>\n",
       "      <td>0.950276</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.102767</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.230022</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.393258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036664</td>\n",
       "      <td>0.618785</td>\n",
       "      <td>0.370690</td>\n",
       "      <td>0.197628</td>\n",
       "      <td>0.006771</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.573034</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.037580</td>\n",
       "      <td>0.651934</td>\n",
       "      <td>0.491379</td>\n",
       "      <td>0.128458</td>\n",
       "      <td>0.008406</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age  Gender  Heart rate  Systolic blood pressure  \\\n",
       "0  0.550562     1.0    0.042163                 0.651934   \n",
       "1  0.067416     1.0    0.067828                 0.309392   \n",
       "2  0.471910     1.0    0.040330                 0.651934   \n",
       "3  0.584270     1.0    0.045830                 0.430939   \n",
       "4  0.449438     1.0    0.040330                 0.386740   \n",
       "5  0.426966     0.0    0.037580                 0.386740   \n",
       "6  0.269663     0.0    0.018332                 0.756906   \n",
       "7  0.528090     1.0    0.036664                 0.950276   \n",
       "8  0.393258     0.0    0.036664                 0.618785   \n",
       "9  0.573034     1.0    0.037580                 0.651934   \n",
       "\n",
       "   Diastolic blood pressure  Blood sugar     CK-MB  Troponin  Result  \n",
       "0                  0.387931     0.247036  0.004935  0.001068     0.0  \n",
       "1                  0.068966     0.515810  0.021453  0.102826     1.0  \n",
       "2                  0.336207     0.464427  0.005569  0.000194     0.0  \n",
       "3                  0.146552     0.464427  0.045212  0.011749     1.0  \n",
       "4                  0.232759     0.523715  0.002533  0.000194     0.0  \n",
       "5                  0.172414     0.102767  0.005035  0.000291     0.0  \n",
       "6                  0.258621     0.132411  0.001298  0.000194     0.0  \n",
       "7                  0.379310     0.102767  1.000000  0.230022     1.0  \n",
       "8                  0.370690     0.197628  0.006771  0.000291     0.0  \n",
       "9                  0.491379     0.128458  0.008406  0.000971     0.0  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_medical_data['Result'] = df_medical_data['Result'].replace({'negative': 0, 'positive': 1})\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df_medical_data[['Age', 'Gender', 'Heart rate', 'Systolic blood pressure', 'Diastolic blood pressure', 'Blood sugar', 'CK-MB', 'Troponin', 'Result']] = scaler.fit_transform(df_medical_data[['Age', 'Gender', 'Heart rate', 'Systolic blood pressure', 'Diastolic blood pressure', 'Blood sugar', 'CK-MB', 'Troponin', 'Result']])\n",
    "\n",
    "df_medical_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Begin splitting data and forming training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(791, 8) (791, 2) (264, 8) (264, 2) (264, 8) (264, 2)\n"
     ]
    }
   ],
   "source": [
    "#Split dataframe into input and output sets.\n",
    "y = df_medical_data['Result'] \n",
    "X = df_medical_data.drop('Result', axis=1)\n",
    "\n",
    "y = to_categorical(y) #one-hot encoding as opposed to simply using 0 or 1\n",
    "\n",
    "# random_state=42 makes sures the RNG used by train_test_split is initialized to the same state everytime and ensures that the data is always split the same way\n",
    "# Ratio split is Train:Validation:Test = 60:20:20\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, stratify=df_medical_data['Result'], random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape) #Ensuring the size of the input and output for each set match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Reece Lazarus\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - accuracy: 0.5632 - loss: 0.6828 - val_accuracy: 0.6136 - val_loss: 0.6651\n",
      "Epoch 2/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6290 - loss: 0.6465 - val_accuracy: 0.6136 - val_loss: 0.6472\n",
      "Epoch 3/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6380 - loss: 0.6152 - val_accuracy: 0.6136 - val_loss: 0.6099\n",
      "Epoch 4/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6409 - loss: 0.5874 - val_accuracy: 0.6818 - val_loss: 0.6074\n",
      "Epoch 5/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7124 - loss: 0.5330 - val_accuracy: 0.6818 - val_loss: 0.6099\n",
      "Epoch 6/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7267 - loss: 0.5226 - val_accuracy: 0.7273 - val_loss: 0.5679\n",
      "Epoch 7/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7249 - loss: 0.4935 - val_accuracy: 0.7462 - val_loss: 0.5557\n",
      "Epoch 8/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7528 - loss: 0.4681 - val_accuracy: 0.6742 - val_loss: 0.7306\n",
      "Epoch 9/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7530 - loss: 0.4914 - val_accuracy: 0.7538 - val_loss: 0.5624\n",
      "Epoch 10/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7959 - loss: 0.4135 - val_accuracy: 0.7614 - val_loss: 0.5537\n",
      "Epoch 11/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7951 - loss: 0.4125 - val_accuracy: 0.7083 - val_loss: 0.6168\n",
      "Epoch 12/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7997 - loss: 0.4131 - val_accuracy: 0.7576 - val_loss: 0.5846\n",
      "Epoch 13/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8003 - loss: 0.3797 - val_accuracy: 0.7576 - val_loss: 0.5844\n",
      "Epoch 14/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7911 - loss: 0.3922 - val_accuracy: 0.7689 - val_loss: 0.5961\n",
      "Epoch 15/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8096 - loss: 0.3711 - val_accuracy: 0.7576 - val_loss: 0.6805\n",
      "Epoch 16/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8149 - loss: 0.3791 - val_accuracy: 0.7462 - val_loss: 0.6348\n",
      "Epoch 17/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8020 - loss: 0.3652 - val_accuracy: 0.7765 - val_loss: 0.6100\n",
      "Epoch 18/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8152 - loss: 0.3592 - val_accuracy: 0.7652 - val_loss: 0.6380\n",
      "Epoch 19/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8156 - loss: 0.3578 - val_accuracy: 0.7197 - val_loss: 0.6740\n",
      "Epoch 20/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8197 - loss: 0.3471 - val_accuracy: 0.7121 - val_loss: 0.7246\n",
      "Epoch 21/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8090 - loss: 0.3690 - val_accuracy: 0.7803 - val_loss: 0.6716\n",
      "Epoch 22/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8056 - loss: 0.3804 - val_accuracy: 0.7992 - val_loss: 0.6427\n",
      "Epoch 23/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8243 - loss: 0.3889 - val_accuracy: 0.7917 - val_loss: 0.6248\n",
      "Epoch 24/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8504 - loss: 0.3110 - val_accuracy: 0.7992 - val_loss: 0.6431\n",
      "Epoch 25/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8587 - loss: 0.3170 - val_accuracy: 0.7955 - val_loss: 0.6741\n",
      "Epoch 26/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8512 - loss: 0.3176 - val_accuracy: 0.7803 - val_loss: 0.7217\n",
      "Epoch 27/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8279 - loss: 0.3480 - val_accuracy: 0.8068 - val_loss: 0.7113\n",
      "Epoch 28/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8519 - loss: 0.3148 - val_accuracy: 0.7538 - val_loss: 0.8003\n",
      "Epoch 29/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8077 - loss: 0.3663 - val_accuracy: 0.8295 - val_loss: 0.7160\n",
      "Epoch 30/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8216 - loss: 0.3321 - val_accuracy: 0.7765 - val_loss: 0.7561\n",
      "Epoch 31/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8501 - loss: 0.3108 - val_accuracy: 0.7765 - val_loss: 0.8016\n",
      "Epoch 32/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8575 - loss: 0.3183 - val_accuracy: 0.7917 - val_loss: 0.7645\n",
      "Epoch 33/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8133 - loss: 0.3827 - val_accuracy: 0.7879 - val_loss: 0.7592\n",
      "Epoch 34/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8436 - loss: 0.3118 - val_accuracy: 0.7955 - val_loss: 0.7415\n",
      "Epoch 35/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8440 - loss: 0.3020 - val_accuracy: 0.7689 - val_loss: 0.8378\n",
      "Epoch 36/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8160 - loss: 0.3655 - val_accuracy: 0.8106 - val_loss: 0.7697\n",
      "Epoch 37/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8288 - loss: 0.3294 - val_accuracy: 0.7955 - val_loss: 0.7683\n",
      "Epoch 38/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8581 - loss: 0.3067 - val_accuracy: 0.8333 - val_loss: 0.7804\n",
      "Epoch 39/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8554 - loss: 0.3016 - val_accuracy: 0.7879 - val_loss: 0.8441\n",
      "Epoch 40/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8507 - loss: 0.2845 - val_accuracy: 0.8295 - val_loss: 0.8320\n",
      "Epoch 41/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8627 - loss: 0.2861 - val_accuracy: 0.7955 - val_loss: 0.8906\n",
      "Epoch 42/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8547 - loss: 0.3001 - val_accuracy: 0.7917 - val_loss: 0.9443\n",
      "Epoch 43/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8458 - loss: 0.2811 - val_accuracy: 0.8144 - val_loss: 0.9630\n",
      "Epoch 44/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8230 - loss: 0.3318 - val_accuracy: 0.7803 - val_loss: 0.9696\n",
      "Epoch 45/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8688 - loss: 0.2816 - val_accuracy: 0.7879 - val_loss: 0.9670\n",
      "Epoch 46/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8652 - loss: 0.2762 - val_accuracy: 0.8182 - val_loss: 0.9725\n",
      "Epoch 47/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8835 - loss: 0.2457 - val_accuracy: 0.7841 - val_loss: 1.0935\n",
      "Epoch 48/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8575 - loss: 0.2826 - val_accuracy: 0.8030 - val_loss: 1.0410\n",
      "Epoch 49/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8408 - loss: 0.3141 - val_accuracy: 0.8030 - val_loss: 1.0431\n",
      "Epoch 50/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8710 - loss: 0.2817 - val_accuracy: 0.8220 - val_loss: 1.0231\n",
      "Epoch 51/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8423 - loss: 0.2847 - val_accuracy: 0.7992 - val_loss: 1.0932\n",
      "Epoch 52/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8777 - loss: 0.2660 - val_accuracy: 0.7841 - val_loss: 1.1771\n",
      "Epoch 53/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8239 - loss: 0.3692 - val_accuracy: 0.8258 - val_loss: 1.0246\n",
      "Epoch 54/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8638 - loss: 0.2815 - val_accuracy: 0.7955 - val_loss: 1.1070\n",
      "Epoch 55/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8805 - loss: 0.2522 - val_accuracy: 0.8030 - val_loss: 1.1536\n",
      "Epoch 56/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8757 - loss: 0.2421 - val_accuracy: 0.8220 - val_loss: 1.1573\n",
      "Epoch 57/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8967 - loss: 0.2509 - val_accuracy: 0.8333 - val_loss: 1.1893\n",
      "Epoch 58/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9014 - loss: 0.2105 - val_accuracy: 0.8144 - val_loss: 1.2421\n",
      "Epoch 59/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8730 - loss: 0.2445 - val_accuracy: 0.7955 - val_loss: 1.3555\n",
      "Epoch 60/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8419 - loss: 0.3131 - val_accuracy: 0.7879 - val_loss: 1.2977\n",
      "Epoch 61/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8904 - loss: 0.2590 - val_accuracy: 0.7917 - val_loss: 1.3845\n",
      "Epoch 62/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8533 - loss: 0.3049 - val_accuracy: 0.7917 - val_loss: 1.3611\n",
      "Epoch 63/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8516 - loss: 0.3077 - val_accuracy: 0.8258 - val_loss: 1.2961\n",
      "Epoch 64/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8618 - loss: 0.2722 - val_accuracy: 0.8295 - val_loss: 1.2693\n",
      "Epoch 65/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8759 - loss: 0.2523 - val_accuracy: 0.8106 - val_loss: 1.3894\n",
      "Epoch 66/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8960 - loss: 0.2236 - val_accuracy: 0.8144 - val_loss: 1.4270\n",
      "Epoch 67/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8720 - loss: 0.2530 - val_accuracy: 0.8371 - val_loss: 1.4828\n",
      "Epoch 68/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9003 - loss: 0.2138 - val_accuracy: 0.8371 - val_loss: 1.5192\n",
      "Epoch 69/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8799 - loss: 0.2306 - val_accuracy: 0.8068 - val_loss: 1.6141\n",
      "Epoch 70/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8973 - loss: 0.2379 - val_accuracy: 0.7879 - val_loss: 1.6372\n",
      "Epoch 71/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8924 - loss: 0.2228 - val_accuracy: 0.8144 - val_loss: 1.6176\n",
      "Epoch 72/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8908 - loss: 0.2312 - val_accuracy: 0.8144 - val_loss: 1.6856\n",
      "Epoch 73/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8966 - loss: 0.2234 - val_accuracy: 0.7462 - val_loss: 1.9609\n",
      "Epoch 74/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8157 - loss: 0.3680 - val_accuracy: 0.7917 - val_loss: 1.5447\n",
      "Epoch 75/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8842 - loss: 0.2566 - val_accuracy: 0.8371 - val_loss: 1.5671\n",
      "Epoch 76/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8995 - loss: 0.2371 - val_accuracy: 0.8220 - val_loss: 1.6470\n",
      "Epoch 77/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9070 - loss: 0.2091 - val_accuracy: 0.8295 - val_loss: 1.6892\n",
      "Epoch 78/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9239 - loss: 0.1860 - val_accuracy: 0.8144 - val_loss: 1.7570\n",
      "Epoch 79/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8962 - loss: 0.2319 - val_accuracy: 0.8144 - val_loss: 1.8681\n",
      "Epoch 80/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9021 - loss: 0.2058 - val_accuracy: 0.8182 - val_loss: 1.8987\n",
      "Epoch 81/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8951 - loss: 0.2046 - val_accuracy: 0.8295 - val_loss: 1.8589\n",
      "Epoch 82/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9078 - loss: 0.2072 - val_accuracy: 0.8182 - val_loss: 1.9535\n",
      "Epoch 83/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9136 - loss: 0.2012 - val_accuracy: 0.8371 - val_loss: 2.0148\n",
      "Epoch 84/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9063 - loss: 0.2066 - val_accuracy: 0.7992 - val_loss: 2.1078\n",
      "Epoch 85/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9009 - loss: 0.2318 - val_accuracy: 0.8182 - val_loss: 2.0060\n",
      "Epoch 86/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8915 - loss: 0.2082 - val_accuracy: 0.8523 - val_loss: 2.0434\n",
      "Epoch 87/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9164 - loss: 0.1815 - val_accuracy: 0.8409 - val_loss: 2.0866\n",
      "Epoch 88/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9283 - loss: 0.1884 - val_accuracy: 0.8409 - val_loss: 2.2090\n",
      "Epoch 89/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9153 - loss: 0.2010 - val_accuracy: 0.8144 - val_loss: 2.2964\n",
      "Epoch 90/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9206 - loss: 0.1883 - val_accuracy: 0.8258 - val_loss: 2.3033\n",
      "Epoch 91/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9216 - loss: 0.1809 - val_accuracy: 0.8371 - val_loss: 2.2934\n",
      "Epoch 92/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8973 - loss: 0.2127 - val_accuracy: 0.8333 - val_loss: 2.3207\n",
      "Epoch 93/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9085 - loss: 0.1987 - val_accuracy: 0.8485 - val_loss: 2.3256\n",
      "Epoch 94/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9389 - loss: 0.1639 - val_accuracy: 0.8182 - val_loss: 2.3995\n",
      "Epoch 95/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8960 - loss: 0.2201 - val_accuracy: 0.8295 - val_loss: 2.3748\n",
      "Epoch 96/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9003 - loss: 0.1905 - val_accuracy: 0.8333 - val_loss: 2.4862\n",
      "Epoch 97/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9252 - loss: 0.1535 - val_accuracy: 0.8258 - val_loss: 2.5886\n",
      "Epoch 98/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8669 - loss: 0.2681 - val_accuracy: 0.7841 - val_loss: 2.4837\n",
      "Epoch 99/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9029 - loss: 0.2094 - val_accuracy: 0.8144 - val_loss: 2.5166\n",
      "Epoch 100/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9161 - loss: 0.1838 - val_accuracy: 0.8447 - val_loss: 2.5511\n",
      "Epoch 101/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9204 - loss: 0.1739 - val_accuracy: 0.8409 - val_loss: 2.6788\n",
      "Epoch 102/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9267 - loss: 0.1699 - val_accuracy: 0.8144 - val_loss: 2.7505\n",
      "Epoch 103/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9120 - loss: 0.1722 - val_accuracy: 0.8409 - val_loss: 2.7491\n",
      "Epoch 104/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9111 - loss: 0.1784 - val_accuracy: 0.8523 - val_loss: 2.7024\n",
      "Epoch 105/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9269 - loss: 0.1688 - val_accuracy: 0.8295 - val_loss: 2.7844\n",
      "Epoch 106/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9117 - loss: 0.1877 - val_accuracy: 0.8485 - val_loss: 2.8715\n",
      "Epoch 107/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9055 - loss: 0.1868 - val_accuracy: 0.8447 - val_loss: 2.8564\n",
      "Epoch 108/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9272 - loss: 0.1651 - val_accuracy: 0.7424 - val_loss: 3.0689\n",
      "Epoch 109/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8785 - loss: 0.2730 - val_accuracy: 0.8030 - val_loss: 2.8030\n",
      "Epoch 110/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9096 - loss: 0.2048 - val_accuracy: 0.8409 - val_loss: 2.7075\n",
      "Epoch 111/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9175 - loss: 0.1716 - val_accuracy: 0.8523 - val_loss: 2.8188\n",
      "Epoch 112/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9333 - loss: 0.1562 - val_accuracy: 0.8333 - val_loss: 2.9861\n",
      "Epoch 113/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9135 - loss: 0.1767 - val_accuracy: 0.8409 - val_loss: 3.0069\n",
      "Epoch 114/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9204 - loss: 0.1730 - val_accuracy: 0.8333 - val_loss: 3.0035\n",
      "Epoch 115/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9211 - loss: 0.1787 - val_accuracy: 0.8333 - val_loss: 2.9404\n",
      "Epoch 116/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9263 - loss: 0.1623 - val_accuracy: 0.8485 - val_loss: 3.1262\n",
      "Epoch 117/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9285 - loss: 0.1622 - val_accuracy: 0.8447 - val_loss: 3.1466\n",
      "Epoch 118/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9276 - loss: 0.1633 - val_accuracy: 0.8447 - val_loss: 3.2204\n",
      "Epoch 119/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9206 - loss: 0.1779 - val_accuracy: 0.8295 - val_loss: 3.2530\n",
      "Epoch 120/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9320 - loss: 0.1548 - val_accuracy: 0.8371 - val_loss: 3.3170\n",
      "Epoch 121/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9233 - loss: 0.1762 - val_accuracy: 0.8333 - val_loss: 3.2184\n",
      "Epoch 122/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9222 - loss: 0.1832 - val_accuracy: 0.8485 - val_loss: 3.1815\n",
      "Epoch 123/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9431 - loss: 0.1487 - val_accuracy: 0.8485 - val_loss: 3.3288\n",
      "Epoch 124/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9389 - loss: 0.1524 - val_accuracy: 0.8447 - val_loss: 3.4550\n",
      "Epoch 125/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9360 - loss: 0.1528 - val_accuracy: 0.8485 - val_loss: 3.6123\n",
      "Epoch 126/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9422 - loss: 0.1390 - val_accuracy: 0.8371 - val_loss: 3.5798\n",
      "Epoch 127/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9262 - loss: 0.1398 - val_accuracy: 0.8409 - val_loss: 3.6534\n",
      "Epoch 128/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9366 - loss: 0.1428 - val_accuracy: 0.8258 - val_loss: 3.7885\n",
      "Epoch 129/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9081 - loss: 0.1734 - val_accuracy: 0.8485 - val_loss: 3.6556\n",
      "Epoch 130/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9300 - loss: 0.1494 - val_accuracy: 0.8295 - val_loss: 3.8079\n",
      "Epoch 131/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9243 - loss: 0.1567 - val_accuracy: 0.8485 - val_loss: 3.7048\n",
      "Epoch 132/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8842 - loss: 0.2007 - val_accuracy: 0.8636 - val_loss: 3.6303\n",
      "Epoch 133/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9445 - loss: 0.1318 - val_accuracy: 0.8523 - val_loss: 3.7992\n",
      "Epoch 134/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9353 - loss: 0.1465 - val_accuracy: 0.8409 - val_loss: 3.8340\n",
      "Epoch 135/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9292 - loss: 0.1468 - val_accuracy: 0.8485 - val_loss: 3.7394\n",
      "Epoch 136/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9342 - loss: 0.1385 - val_accuracy: 0.8674 - val_loss: 3.7770\n",
      "Epoch 137/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9494 - loss: 0.1362 - val_accuracy: 0.8409 - val_loss: 3.8736\n",
      "Epoch 138/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9204 - loss: 0.1606 - val_accuracy: 0.8409 - val_loss: 4.0135\n",
      "Epoch 139/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9405 - loss: 0.1376 - val_accuracy: 0.8333 - val_loss: 3.9434\n",
      "Epoch 140/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9580 - loss: 0.1086 - val_accuracy: 0.8371 - val_loss: 4.0506\n",
      "Epoch 141/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8981 - loss: 0.1857 - val_accuracy: 0.8220 - val_loss: 4.0374\n",
      "Epoch 142/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9228 - loss: 0.1727 - val_accuracy: 0.8409 - val_loss: 3.9739\n",
      "Epoch 143/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9369 - loss: 0.1405 - val_accuracy: 0.8333 - val_loss: 4.2173\n",
      "Epoch 144/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9382 - loss: 0.1493 - val_accuracy: 0.8636 - val_loss: 4.2708\n",
      "Epoch 145/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9284 - loss: 0.1644 - val_accuracy: 0.8636 - val_loss: 4.2053\n",
      "Epoch 146/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9431 - loss: 0.1308 - val_accuracy: 0.8447 - val_loss: 4.3195\n",
      "Epoch 147/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9412 - loss: 0.1285 - val_accuracy: 0.8826 - val_loss: 4.4202\n",
      "Epoch 148/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9664 - loss: 0.1102 - val_accuracy: 0.8295 - val_loss: 4.6283\n",
      "Epoch 149/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9355 - loss: 0.1538 - val_accuracy: 0.8636 - val_loss: 4.5362\n",
      "Epoch 150/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9488 - loss: 0.1167 - val_accuracy: 0.8788 - val_loss: 4.5425\n",
      "Epoch 151/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9128 - loss: 0.1858 - val_accuracy: 0.8598 - val_loss: 4.3364\n",
      "Epoch 152/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9229 - loss: 0.1631 - val_accuracy: 0.8485 - val_loss: 4.4039\n",
      "Epoch 153/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9296 - loss: 0.1411 - val_accuracy: 0.8371 - val_loss: 4.4927\n",
      "Epoch 154/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9108 - loss: 0.1774 - val_accuracy: 0.8485 - val_loss: 4.2482\n",
      "Epoch 155/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9530 - loss: 0.1344 - val_accuracy: 0.8598 - val_loss: 4.2665\n",
      "Epoch 156/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9476 - loss: 0.1226 - val_accuracy: 0.8523 - val_loss: 4.4552\n",
      "Epoch 157/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9497 - loss: 0.1136 - val_accuracy: 0.8523 - val_loss: 4.6352\n",
      "Epoch 158/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9569 - loss: 0.0901 - val_accuracy: 0.8788 - val_loss: 4.8171\n",
      "Epoch 159/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9618 - loss: 0.0989 - val_accuracy: 0.8485 - val_loss: 5.0243\n",
      "Epoch 160/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9390 - loss: 0.1258 - val_accuracy: 0.8447 - val_loss: 4.9827\n",
      "Epoch 161/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9652 - loss: 0.1000 - val_accuracy: 0.8826 - val_loss: 5.0454\n",
      "Epoch 162/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9428 - loss: 0.1142 - val_accuracy: 0.8750 - val_loss: 5.0863\n",
      "Epoch 163/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9531 - loss: 0.1219 - val_accuracy: 0.8485 - val_loss: 5.0545\n",
      "Epoch 164/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9451 - loss: 0.1453 - val_accuracy: 0.8826 - val_loss: 5.0401\n",
      "Epoch 165/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9379 - loss: 0.1090 - val_accuracy: 0.8864 - val_loss: 5.1008\n",
      "Epoch 166/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9101 - loss: 0.1733 - val_accuracy: 0.8447 - val_loss: 4.9891\n",
      "Epoch 167/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9270 - loss: 0.1495 - val_accuracy: 0.8750 - val_loss: 4.8406\n",
      "Epoch 168/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9420 - loss: 0.1314 - val_accuracy: 0.8902 - val_loss: 4.8074\n",
      "Epoch 169/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9573 - loss: 0.1301 - val_accuracy: 0.8674 - val_loss: 4.9519\n",
      "Epoch 170/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9494 - loss: 0.1118 - val_accuracy: 0.8598 - val_loss: 5.0444\n",
      "Epoch 171/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9457 - loss: 0.1255 - val_accuracy: 0.8864 - val_loss: 5.0891\n",
      "Epoch 172/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9362 - loss: 0.1170 - val_accuracy: 0.8447 - val_loss: 5.2177\n",
      "Epoch 173/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9521 - loss: 0.1006 - val_accuracy: 0.8523 - val_loss: 5.3391\n",
      "Epoch 174/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9259 - loss: 0.1509 - val_accuracy: 0.8447 - val_loss: 5.2266\n",
      "Epoch 175/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9519 - loss: 0.1183 - val_accuracy: 0.8788 - val_loss: 5.3372\n",
      "Epoch 176/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8986 - loss: 0.2042 - val_accuracy: 0.8447 - val_loss: 5.0680\n",
      "Epoch 177/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9499 - loss: 0.1237 - val_accuracy: 0.8636 - val_loss: 5.1942\n",
      "Epoch 178/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9658 - loss: 0.0934 - val_accuracy: 0.8409 - val_loss: 5.5527\n",
      "Epoch 179/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9442 - loss: 0.1116 - val_accuracy: 0.8674 - val_loss: 5.5561\n",
      "Epoch 180/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9652 - loss: 0.0953 - val_accuracy: 0.8598 - val_loss: 5.7174\n",
      "Epoch 181/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8989 - loss: 0.2472 - val_accuracy: 0.8333 - val_loss: 5.2473\n",
      "Epoch 182/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9028 - loss: 0.2143 - val_accuracy: 0.8712 - val_loss: 4.8480\n",
      "Epoch 183/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9521 - loss: 0.1137 - val_accuracy: 0.8750 - val_loss: 5.0121\n",
      "Epoch 184/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9511 - loss: 0.1108 - val_accuracy: 0.8864 - val_loss: 5.2338\n",
      "Epoch 185/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9629 - loss: 0.1018 - val_accuracy: 0.8788 - val_loss: 5.4164\n",
      "Epoch 186/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9539 - loss: 0.0948 - val_accuracy: 0.8561 - val_loss: 5.5888\n",
      "Epoch 187/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9457 - loss: 0.1238 - val_accuracy: 0.8788 - val_loss: 5.6473\n",
      "Epoch 188/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9491 - loss: 0.1152 - val_accuracy: 0.8826 - val_loss: 5.7342\n",
      "Epoch 189/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9448 - loss: 0.1031 - val_accuracy: 0.8864 - val_loss: 5.8757\n",
      "Epoch 190/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9508 - loss: 0.1086 - val_accuracy: 0.8939 - val_loss: 5.9863\n",
      "Epoch 191/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9733 - loss: 0.0687 - val_accuracy: 0.8902 - val_loss: 6.1898\n",
      "Epoch 192/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9489 - loss: 0.1140 - val_accuracy: 0.8258 - val_loss: 6.4117\n",
      "Epoch 193/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9122 - loss: 0.2442 - val_accuracy: 0.8788 - val_loss: 5.6019\n",
      "Epoch 194/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9554 - loss: 0.1054 - val_accuracy: 0.8750 - val_loss: 5.8121\n",
      "Epoch 195/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9670 - loss: 0.0850 - val_accuracy: 0.8788 - val_loss: 6.0349\n",
      "Epoch 196/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9764 - loss: 0.0757 - val_accuracy: 0.8788 - val_loss: 6.1547\n",
      "Epoch 197/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9425 - loss: 0.1186 - val_accuracy: 0.8750 - val_loss: 6.0516\n",
      "Epoch 198/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9479 - loss: 0.1133 - val_accuracy: 0.8712 - val_loss: 6.0215\n",
      "Epoch 199/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9122 - loss: 0.2131 - val_accuracy: 0.8939 - val_loss: 5.3817\n",
      "Epoch 200/200\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9609 - loss: 0.1217 - val_accuracy: 0.8788 - val_loss: 5.5681\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x20f597af650>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_dim=X_train.shape[1])) #input_dim=X_train.shape[1]\n",
    "model.add(Dense(64, activation='relu')) \n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu')) \n",
    "model.add(Dense(2, activation='softmax')) #2 nodes used in output layer because we're doing binary classification \n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=32, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9372 - loss: 0.4613 \n",
      "\n",
      "Test accuracy: 0.9318181872367859\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test,  y_test)\n",
    "\n",
    "print('\\nTest accuracy:', test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
